Topic: Tensor flow and numpy collab


import tensorflow as tf
import numpy as np
# Creating a variable
variable = tf.Variable([1, 2, 3, 4, 5])
print(f"Variable tensor: {variable}")
print(f"Shape: {variable.shape}")  # Shape: (5,)

# Modifying a variable
variable.assign([5, 4, 3, 2, 1])
print(f"Modified variable: {variable}")

# Creating a variable with a specific shape and data type
zeros_var = tf.Variable(tf.zeros([2, 3], dtype=tf.float32))
print(f"Zeros variable: {zeros_var}")
print(f"Shape: {zeros_var.shape}")  # Shape: (2, 3)

# Updating specific elements
zeros_var[0, 1].assign(5.0)
print(f"Updated zeros variable: {zeros_var}")
























Topic: Tensorflow and Numpy Collab


import tensorflow as tf
import numpy as np

# Creating a scalar (rank-0 tensor)
scalar = tf.constant(42)
print(f"Scalar tensor: {scalar}")
print(f"Shape: {scalar.shape}")  # Shape: ()

# Creating a vector (rank-1 tensor)
vector = tf.constant([1, 2, 3, 4])
print(f"Vector tensor: {vector}")
print(f"Shape: {vector.shape}")  # Shape: (4,)

# Creating a matrix (rank-2 tensor)
matrix = tf.constant([[1, 2, 3], [4, 5, 6]])
print(f"Matrix tensor: {matrix}")
print(f"Shape: {matrix.shape}")  # Shape: (2, 3)

# Creating a 3D tensor
tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print(f"3D tensor: {tensor_3d}")
print(f"Shape: {tensor_3d.shape}")  # Shape: (2, 2, 2)

# Creating tensors with specific data types
float_tensor = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
int_tensor = tf.constant([1, 2, 3], dtype=tf.int32)
bool_tensor = tf.constant([True, False, True], dtype=tf.bool)

















































Topic: Tensorflow and Numpy Collab

import numpy as np
import tensorflow as tf
# Getting tensor shape info
a = tf.constant([[1, 2, 3], [4, 5, 6]])
print(f"Tensor: {a}")
print(f"Shape: {a.shape}")  # TensorShape([2, 3])
print(f"Rank: {tf.rank(a)}")  # 2
print(f"Size: {tf.size(a)}")  # 6

# Reshaping tensors
reshaped = tf.reshape(a, [3, 2])
print(f"Reshaped tensor: {reshaped}")
print(f"New shape: {reshaped.shape}")  # TensorShape([3, 2])

# Use -1 for automatic dimension calculation
auto_reshape = tf.reshape(a, [1, -1])  # Flattens into a 1×6 tensor
print(f"Auto-reshaped tensor: {auto_reshape}")
print(f"New shape: {auto_reshape.shape}")  # TensorShape([1, 6])

# Expanding dimensions
expanded = tf.expand_dims(a, axis=0)
print(f"Expanded tensor: {expanded}")
print(f"New shape: {expanded.shape}")  # TensorShape([1, 2, 3])
































































Topic: Broadcasting with Tensorflow

import tensorflow as tf
# Broadcasting example
matrix = tf.constant([[1, 2], [3, 4], [5, 6]])  # Shape: (3, 2)
vector = tf.constant([10, 20])                  # Shape: (2,)
result = matrix + vector
print(f"Matrix shape: {matrix.shape}")
print(f"Vector shape: {vector.shape}")
print(f"Result after broadcasting: {result}")
print(f"Result shape: {result.shape}")  # Shape: (3, 2)

# Broadcasting rules demonstration
a = tf.constant([[1, 2, 3]])  # Shape: (1, 3)
b = tf.constant([[4], [5], [6]])  # Shape: (3, 1)
c = a + b  # Broadcasting happens
print(f"Broadcasting result: \n{c}")
print(f"Result shape: {c.shape}")  # Shape: (3, 3)

# Squeezing dimensions (removing dimensions of size 1)
squeezed = tf.squeeze(expanded)
print(f"Squeezed tensor: {squeezed}")
print(f"New shape: {squeezed.shape}")  # Back to TensorShape([2, 3])

# Transposing
transposed = tf.transpose(a)
print(f"Transposed tensor: {transposed}")
print(f"New shape: {transposed.shape}")  # TensorShape([3, 2])

































Topic: Tensorflow and Numpy Collab

import tensorflow as tf
import numpy as np
# NumPy array to TensorFlow tensor
numpy_array = np.array([[1, 2, 3], [4, 5, 6]])
tensor_from_np = tf.convert_to_tensor(numpy_array)
print(f"NumPy array: \n{numpy_array}")
print(f"TensorFlow tensor: \n{tensor_from_np}")

# TensorFlow tensor to NumPy array
tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
numpy_from_tf = tensor.numpy()  # or np.array(tensor)
print(f"TensorFlow tensor: \n{tensor}")
print(f"NumPy array: \n{numpy_from_tf}")

# Memory sharing
# Changes to the NumPy array can affect the tensor
numpy_array = np.array([[1, 2, 3], [4, 5, 6]])
tensor = tf.convert_to_tensor(numpy_array)
print(f"Original NumPy array: \n{numpy_array}")
print(f"Original tensor: \n{tensor}")

# Modify the NumPy array
numpy_array[0, 0] = 99
# This won't affect the tensor since tf.convert_to_tensor makes a copy
print(f"Modified NumPy array: \n{numpy_array}")
print(f"Tensor after NumPy modification: \n{tensor}")

# However, tensor.numpy() can share memory with the original tensor in some cases
tensor_var = tf.Variable([[1, 2], [3, 4]])
numpy_view = tensor_var.numpy()
tensor_var.assign([[5, 6], [7, 8]])
print(f"Modified tensor: \n{tensor_var}")
print(f"NumPy view after tensor modification: \n{numpy_view}")  # May or may not reflect changes depending on implementation






























































Topic: Tensorflow Data Checks, and printings.

import tensorflow as tf
# Common TensorFlow data types
print("Common TensorFlow data types:")
print(f"- tf.float32: {tf.constant(1.0, dtype=tf.float32)}")
print(f"- tf.float64: {tf.constant(1.0, dtype=tf.float64)}")
print(f"- tf.int32: {tf.constant(1, dtype=tf.int32)}")
print(f"- tf.int64: {tf.constant(1, dtype=tf.int64)}")
print(f"- tf.bool: {tf.constant(True, dtype=tf.bool)}")
print(f"- tf.string: {tf.constant('Hello', dtype=tf.string)}")
print(f"- tf.complex64: {tf.constant(1+2j, dtype=tf.complex64)}")

# Checking data types
tensor = tf.constant([1, 2, 3])
print(f"Tensor: {tensor}")
print(f"Data type: {tensor.dtype}")

# Type conversion
float_tensor = tf.cast(tensor, dtype=tf.float32)
print(f"After casting: {float_tensor}")
print(f"New data type: {float_tensor.dtype}")

# Data type compatibility
a = tf.constant([1, 2, 3], dtype=tf.int32)
b = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)

# This would fail due to type mismatch
try:
    # c = tf.concat([a, b], axis=0)  # This would cause an error
    pass
except tf.errors.InvalidArgumentError as e:
    print(f"Error: {e}")

# Convert types to make them compatible
a_float = tf.cast(a, dtype=tf.float32)
c = tf.concat([a_float, b], axis=0)
print(f"Concatenated after type casting: {c}")

# Default types
print(f"Default float type: {tf.constant(1.0).dtype}")  # Usually tf.float32
print(f"Default integer type: {tf.constant(1).dtype}")  # Usually tf.int32






























































Topic: Tensorflow and numpy collab mix.

import numpy as np
import tensorflow as tf

# Creating tensors
tensor_1d = tf.constant([1, 2, 3])
tensor_2d = tf.constant([[1, 2], [3, 4]])

# Basic operations
addition = tensor_2d + 5
multiplication = tensor_2d * 3
matrix_multiplication = tf.matmul(tensor_2d, tensor_2d)

print("Original tensor:", tensor_2d.numpy())
print("Addition:", addition.numpy())
print("Multiplication:", multiplication.numpy())
print("Matrix multiplication:", matrix_multiplication.numpy())




# This is for matrix multiplication. 
# [a, b] × [e, f] = [a*e + b*g, a*f + b*h]
# [c, d]   [g, h]   [c*e + d*g, c*f + d*h]

# [1, 2] × [1, 2] = [1*1 + 2*3, 1*2 + 2*4] = [1+6, 2+8] = [7, 10]
# [3, 4]   [3, 4]   [3*1 + 4*3, 3*2 + 4*4] = [3+12, 6+16] = [15, 22]














































Topic: Tensorflow model stacking 

# Import necessary libraries
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
# Create a simple model with Dense layers
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid') # 0.1 It means unlikely but 0.9 its means very likely
])

# Examine the model
model.summary()



















































Topic: Tensorflow CNN Model

import tensorflow as tf
from tensorflow import keras
import numpy as np
# Create a simple CNN model
cnn_model = keras.Sequential([
    keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    keras.layers.Flatten(), # 5x5x64 = 1600 values or numbers.
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax') # (0.1, 0.05, 0.7)
])

# Examine the model
cnn_model.summary()




# Layer names: Automatically named conv2d, max_pooling2d, etc.
# Output Shape:
# (None, 26, 26, 32): None is the batch size (can vary); 26, 26 is the feature map size after convolution (28−3+1=26 due to 3×3 kernel); 32 is the number of filters.
# (None, 13, 13, 32): After pooling, 26×26 is halved to 13×13.
# (None, 11, 11, 64): Second convolution (13−3+1=11) with 64 filters.
# (None, 5, 5, 64): After pooling, 11×11 is halved to 5×5 (rounded down).
# (None, 1600): Flattened to 5×5×64 = 1600 values.
# (None, 128): Dense layer with 128 neurons.
# (None, 10): Final layer with 10 neurons for 10 classes.
# Param # (number of parameters):
# First Conv2D: (3×3×1×32) + 32 = 288 + 32 = 320 (kernel weights + biases).
# First MaxPooling2D: 0 (no learnable parameters).
# Second Conv2D: (3×3×32×64) + 64 = 18432 + 64 = 18496.
# Second MaxPooling2D: 0.
# Flatten: 0.
# First Dense: (1600×128) + 128 = 204800 + 128 = 204928.
# Second Dense: (128×10) + 10 = 1280 + 10 = 1290.
# Total: 320 + 18496 + 204928 + 1290 = 225034 parameters.
# Parameters: These are the weights (filter values, connections) and biases (adjustments) the model learns during training.
# For beginners: This is like a blueprint of the conveyor belt, showing each station (layer), what it produces (output shape), and how many settings (parameters) it needs to learn.
# Summary for Beginners
# This code creates a Convolutional Neural Network (CNN) using Keras, designed for processing images (e.g., recognizing digits in 28×28 grayscale images, like the MNIST dataset).
# You learned:
# How to build a CNN with keras.Sequential by stacking layers.
# Conv2D layers use filters to detect patterns (like edges or shapes) in images.
# MaxPooling2D layers shrink the data to focus on important features and reduce computation.
# The Flatten layer converts 2D feature maps into a 1D list for dense layers.
# Dense layers combine features to make predictions, with the final layer using softmax to output probabilities for 10 classes (e.g., digits 0–9).
# cnn_model.summary() shows the model’s structure, including layer types, output shapes, and the number of parameters to be learned.
# This model is suitable for tasks like classifying handwritten digits, but the code only builds and displays the model, not trains or uses it.
# Notes
# The input_shape=(28, 28, 1) suggests the model is designed for 28×28 grayscale images (like MNIST digits). Each image must match this shape when training or using the model.
# The model is not compiled or trained here. Compiling (e.g., model.compile) and training (e.g., model.fit) would require additional steps.
# The output shapes in the summary assume no padding in convolutions (padding='valid'), which reduces the spatial size (e.g., 28×28 to 26×26). If padding='same' were used, sizes would stay constant.
# NumPy is imported but not used in this code.
# If you have more questions, want to see how to train this CNN, or need clarification on any part (e.g., convolutions or parameters), let me know!
# 
# 
# 
# 
# 
# 
# 
# 























































Topic: Tensorflow LSTM and GRU modesls.


import tensorflow as tf
from tensorflow import keras
# Create a simple RNN model with LSTM
rnn_model = keras.Sequential([
    keras.layers.LSTM(64, return_sequences=True, input_shape=(10, 5)),
    keras.layers.LSTM(32),
    keras.layers.Dense(10, activation='softmax')
])

# Examine the model
rnn_model.summary()

# Create a model with GRU
gru_model = keras.Sequential([
    keras.layers.GRU(64, return_sequences=True, input_shape=(10, 5)),
    keras.layers.GRU(32),
    keras.layers.Dense(10, activation='softmax')
])

# Examine the model
gru_model.summary()

# Layer names: Automatically named lstm, lstm_1, dense.
Output Shape:
(None, 10, 64): None is the batch size (can vary); 10 is the number of time steps (because return_sequences=True); 64 is the number of units.
(None, 32): Second LSTM outputs 32 values (no sequence, just one vector per sample).
(None, 10): Dense layer outputs 10 probabilities for 10 classes.
Param # (number of parameters):
First LSTM: [(5 inputs + 64 units + 1 bias) × 64 units × 4 gates] = (5 + 64 + 1) × 64 × 4 = 17920. (LSTM has 4 gates: input, forget, cell, output.)
Second LSTM: [(64 inputs + 32 units + 1 bias) × 32 units × 4 gates] = (64 + 32 + 1) × 32 × 4 = 12416.
Dense: (32 inputs × 10 units) + 10 biases = 320 + 10 = 330.
Total: 17920 + 12416 + 330 = 30666 parameters.
Parameters: These are the weights and biases the model learns during training to process sequences and make predictions.
For beginners: This is like a blueprint of the conveyor belt, showing each station (layer), what it produces (output shape), and how many settings (parameters) it needs to learn.

















 Topic: Tensorflow Dropout/Batch Noramlization technique. 

import tensorflow
from tensorflow import keras
# Create a model with dropout layers
dropout_model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)), # RELU example f(x) = max(0, x)
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation='softmax') # What this part does for example For example, if the model is classifying digits, the output might be [0.1, 0.05, 0.6, ..., 0.02], where the highest value (0.6) indicates the predicted digit.
])

# Examine the model
dropout_model.summary()

# Create a model with batch normalization
bn_model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(784,)), # batch of data during training.
# It does two things:
#Scales the outputs so their mean is 0 and variance is 1 (like standardizing data).
#Learns two parameters per neuron (scale and shift) to adjust the normalized values.
    keras.layers.BatchNormalization(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(10, activation='softmax')
])

# Examine the model
bn_model.summary()
